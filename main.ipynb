{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 3\n",
      "GPU 0: NVIDIA A40\n",
      "GPU 1: NVIDIA A40\n",
      "GPU 2: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2,3'\n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    for gpu_id in range(num_gpus):\n",
    "        print(f\"GPU {gpu_id}: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n",
    "\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel( logging.DEBUG )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "hg_legal_model = \"Dhananjayg22/legal-triplet-extractor\"\n",
    "base_model = \"google/gemma-7b-it\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.padding_side = \"right\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                                                hg_legal_model,\n",
    "                                                #quantization_config=bnb_config,\n",
    "                                                device_map=\"auto\",\n",
    "                                                attn_implementation=\"flash_attention_2\",\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                )\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=3000,eos_token_id=107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of small precedents:  300\n"
     ]
    }
   ],
   "source": [
    "from utils.genral import get_precedients,get_precedients_triplets,get_precedients_anon,get_metadata\n",
    "from utils.langchain import parser\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "id2precedents = get_precedients()\n",
    "id2precedents_triplets = get_precedients_triplets()\n",
    "# id2precedents_anon = get_precedients_anon()\n",
    "# metadata = get_metadata()\n",
    "\n",
    "with open('small_precedents_ids.pkl', 'rb') as f:\n",
    "    small_precedents_ids = pickle.load(f)\n",
    "print(\"Number of small precedents: \", len(small_precedents_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 2560.09it/s]\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"dpo-triplets-annon/all\"\n",
    "import json\n",
    "for id in tqdm.tqdm( small_precedents_ids ):\n",
    "    #BEAUTIFY OUTPUT\n",
    "    formatted_json_string = json.dumps(json.loads(id2precedents_triplets[id]), default=lambda o: o.__dict__ , indent=2)  \n",
    "    \n",
    "    with open(f\"{ output_folder }/{ id }.json\", \"w\") as file:\n",
    "        file.write(formatted_json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_annon_doc = id2precedents[small_precedents_ids[100]]\n",
    "# annon_doc = id2precedents_anon[small_precedents_ids[100]]\n",
    "# print(non_annon_doc)\n",
    "# print(annon_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET TRIPLETS FROM NON ANNON PRECEDENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.langchain import langchain_chat_template,parser\n",
    "from utils.genral import clean_string\n",
    "import json\n",
    "import traceback\n",
    "import tqdm\n",
    "\n",
    "output_folder = \"dpo-triplets/all\"\n",
    "parse_error_folder = \"dpo-triplets/parse_failed\"\n",
    "\n",
    "for id in tqdm.tqdm( small_precedents_ids ):\n",
    "\n",
    "    try:\n",
    "        #IF PRESENT SKIP\n",
    "        if os.path.exists(f\"{ output_folder }/{ id }.json\"):\n",
    "            logger.info(f\"Skipping doc_id: {id}\")\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(f\"{ parse_error_folder }/{ id }.json\"):\n",
    "            logger.info(f\"Skipping doc_id: {id} Present in parse folder\")\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Processing doc_id: {id}\")\n",
    "        #GENRATE INPUT PROMPT   \n",
    "        document = id2precedents[id]\n",
    "        #document = tokenizer.decode( tokenizer.encode( document )[1:7000] )\n",
    "\n",
    "        chat_template = langchain_chat_template.format_messages(document=document)\n",
    "        input_prompt = clean_string( chat_template[0].content + chat_template[1].content ) \n",
    "        chat=[\n",
    "            {\"role\": \"user\", \"content\": input_prompt }\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(chat, tokenize=False,add_generation_prompt=True)\n",
    "        logger.info(f\"Promt token length: { len( tokenizer.encode( prompt )) }\")\n",
    "\n",
    "        # if len( tokenizer.encode( prompt )) > 8000:\n",
    "        #     logger.error(f\"Skipping doc_id: {id} due to length\")\n",
    "        #     continue\n",
    "\n",
    "        out = pipe(prompt, return_full_text=False)\n",
    "\n",
    "        #PARSE OUTPUT\n",
    "        try:\n",
    "            response = parser.parse( out[0][\"generated_text\"] )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing doc_id: {id}\")\n",
    "\n",
    "            with open(f\"{ parse_error_folder }/{ id }.json\", \"w\") as file:\n",
    "                file.write(out[0][\"generated_text\"])\n",
    "            continue\n",
    "        \n",
    "        #BEAUTIFY OUTPUT\n",
    "        formatted_json_string = json.dumps(response, default=lambda o: o.__dict__ , indent=2)  \n",
    "        \n",
    "        with open(f\"{ output_folder }/{ id }.json\", \"w\") as file:\n",
    "            file.write(formatted_json_string)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "            logger.error(f\"Error processing doc_id: {id}\")\n",
    "            logger.error( clean_string(traceback.format_exc())[:1000] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIX PARSE FAILED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 168907050.json\n",
      "File: 1900540.json\n",
      "File: 4148349.json\n",
      "File: 37254300.json\n",
      "File: 97604604.json\n",
      "File: 1351401.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from utils.langchain import langchain_chat_template,parser\n",
    "\n",
    "folder_path = \"dpo-triplets/parse_failed\"\n",
    "\n",
    "# Function to read text from a file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Iterate through each file and read its content\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.isfile(file_path):  # Check if it's a file\n",
    "        text = read_file(file_path)\n",
    "        print(f\"File: {file_name}\")\n",
    "        parser.parse(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
